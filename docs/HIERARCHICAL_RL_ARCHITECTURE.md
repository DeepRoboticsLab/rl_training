# ê³„ì¸µì  ê°•í™”í•™ìŠµ (Hierarchical Reinforcement Learning) êµ¬ì¡°

## ğŸ“ ì „ì²´ êµ¬ì¡° ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    High-Level Policy                        â”‚
â”‚              (í•™ìŠµ ëŒ€ìƒ - ìƒˆë¡œ í•™ìŠµí•˜ëŠ” ì •ì±…)                â”‚
â”‚                                                             â”‚
â”‚  Observation: 8D (ë¡œë´‡ ìœ„ì¹˜, ëª©í‘œ ìœ„ì¹˜, ê±°ë¦¬, ë°©í–¥)         â”‚
â”‚  Action: 3D (vx, vy, vyaw) - ì†ë„ ëª…ë ¹                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ Velocity Command [vx, vy, vyaw]
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FrozenLocomotionPolicy (Frozen Policy)              â”‚
â”‚        (ê³ ì •ëœ ì •ì±… - ì´ë¯¸ í•™ìŠµëœ Low-Level Policy)          â”‚
â”‚                                                             â”‚
â”‚  Input: Velocity Command [vx, vy, vyaw]                    â”‚
â”‚  Output: Joint Actions [num_joints]                        â”‚
â”‚                                                             â”‚
â”‚  â€» ì´ ì •ì±…ì€ frozen ìƒíƒœë¡œ, í•™ìŠµë˜ì§€ ì•ŠìŒ                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ Joint Actions
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Low-Level Environment                              â”‚
â”‚    (Isaac Lab - Rough-Deeprobotics-M20-v0)                  â”‚
â”‚                                                             â”‚
â”‚  - ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜                                          â”‚
â”‚  - ë¡œë´‡ ê´€ì ˆ ì œì–´                                          â”‚
â”‚  - ì„¼ì„œ ë°ì´í„°                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ ë°ì´í„° íë¦„ (Step ê³¼ì •)

### 1. High-Level Step

```python
# High-Level Policyê°€ velocity command ìƒì„±
high_level_action = [vx, vy, vyaw]  # ì˜ˆ: [1.0, 0.5, 0.2]
```

### 2. Frozen Policy ë³€í™˜

```python
# FrozenLocomotionPolicyê°€ velocity commandë¥¼ joint actionsë¡œ ë³€í™˜
joint_actions = frozen_policy(high_level_action)
# joint_actions: [num_envs, num_joints] í˜•íƒœ
```

### 3. Low-Level Environment ì‹¤í–‰

```python
# Low-level environmentë¥¼ decimation íšŸìˆ˜ë§Œí¼ ì‹¤í–‰
for _ in range(decimation):  # ê¸°ë³¸ê°’: 10íšŒ
    obs, rewards, dones, extras = low_level_env.step(joint_actions)
```

### 4. High-Level Observation & Reward ê³„ì‚°

```python
# High-level observation ê³„ì‚° (8D)
high_level_obs = [
    robot_pos_x,      # ë¡œë´‡ x ìœ„ì¹˜
    robot_pos_y,      # ë¡œë´‡ y ìœ„ì¹˜
    robot_yaw,        # ë¡œë´‡ ë°©í–¥ (yaw)
    goal_pos_x,       # ëª©í‘œ x ìœ„ì¹˜
    goal_pos_y,       # ëª©í‘œ y ìœ„ì¹˜
    distance,         # ëª©í‘œê¹Œì§€ ê±°ë¦¬
    direction_x,      # ëª©í‘œ ë°©í–¥ (ë¡œë´‡ í”„ë ˆì„ x)
    direction_y,      # ëª©í‘œ ë°©í–¥ (ë¡œë´‡ í”„ë ˆì„ y)
]

# High-level reward ê³„ì‚°
high_level_reward = goal_reaching_reward + 0.5 * progress_reward
```

## ğŸ“Š High-Level MDP (Markov Decision Process)

### Observation Space (8D)

```python
observation_space = Box(
    low=-inf,
    high=inf,
    shape=(8,),
    dtype=float32
)

# êµ¬ì„± ìš”ì†Œ:
# [0:2]   robot_position_2d: ë¡œë´‡ì˜ x, y ìœ„ì¹˜
# [2]     robot_yaw: ë¡œë´‡ì˜ ë°©í–¥ (yaw ê°ë„)
# [3:5]   goal_position_2d: ëª©í‘œì˜ x, y ìœ„ì¹˜
# [5]     distance: ëª©í‘œê¹Œì§€ì˜ ê±°ë¦¬
# [6:8]   direction: ëª©í‘œ ë°©í–¥ (ë¡œë´‡ í”„ë ˆì„ ê¸°ì¤€)
```

### Action Space (3D)

```python
action_space = Box(
    low=[-2.0, -2.0, -2.0],
    high=[2.0, 2.0, 2.0],
    dtype=float32
)

# êµ¬ì„± ìš”ì†Œ:
# [0] vx:   ì•/ë’¤ ì†ë„ (m/s)
# [1] vy:   ì¢Œ/ìš° ì†ë„ (m/s)
# [2] vyaw: íšŒì „ ì†ë„ (rad/s)
```

### Reward Function

```python
# Goal Reaching Reward (Exponential Kernel)
goal_reward = exp(-distance / std^2)  # std = 0.5

# Progress Reward
progress = prev_distance - current_distance

# Total Reward
total_reward = goal_reward + 0.5 * progress
```

### Termination Conditions

```python
# 1. Goal Reached
goal_reached = distance < 0.5  # 0.5m ì´ë‚´ ë„ë‹¬

# 2. Low-Level Done
# Low-level environmentì˜ termination (ì‹œê°„ ì´ˆê³¼, ë‚™ìƒ ë“±)

# High-Level Termination
terminated = goal_reached OR low_level_done
```

## â±ï¸ ì‹œê°„ ìŠ¤ì¼€ì¼ (Decimation)

### Decimation = 10

- **High-Level Step**: 1 step
- **Low-Level Steps**: 10 steps
- **ì˜ë¯¸**: High-level policyê°€ 1ë²ˆì˜ ê²°ì •ì„ ë‚´ë¦¬ë©´, ê·¸ ê²°ì •ì´ low-levelì—ì„œ 10ë²ˆ ì‹¤í–‰ë¨

```
High-Level: [Step 1] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> [Step 2] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> [Step 3]
              â”‚                        â”‚                        â”‚
              â”‚ (decimation=10)        â”‚ (decimation=10)        â”‚
              â–¼                        â–¼                        â–¼
Low-Level:  [1,2,3,...,10]         [11,12,13,...,20]       [21,22,23,...,30]
```

**ì´ìœ **:
- High-levelì€ ì¥ê¸°ì ì¸ ëª©í‘œ(ëª©í‘œ ë„ë‹¬)ì— ì§‘ì¤‘
- Low-levelì€ ë‹¨ê¸°ì ì¸ ì œì–´(ê· í˜•, ë³´í–‰)ì— ì§‘ì¤‘
- Decimationì„ í†µí•´ ì‹œê°„ ìŠ¤ì¼€ì¼ì„ ë¶„ë¦¬

## ğŸ” Frozen Policyì˜ ì—­í• 

### Frozen Policyë€?

```python
# Low-level policyë¥¼ frozen (ê³ ì •) ìƒíƒœë¡œ ì„¤ì •
policy_nn.eval()  # Evaluation ëª¨ë“œ
for param in policy_nn.parameters():
    param.requires_grad = False  # Gradient ê³„ì‚° ë¹„í™œì„±í™”
```

### ì™œ Frozenì¸ê°€?

1. **Low-level policyëŠ” ì´ë¯¸ í•™ìŠµ ì™„ë£Œ**: ë¡œë´‡ì˜ ê¸°ë³¸ ë³´í–‰ ëŠ¥ë ¥ì€ ì´ë¯¸ í•™ìŠµë¨
2. **High-levelë§Œ í•™ìŠµ**: Navigation ì „ëµë§Œ í•™ìŠµí•˜ê¸° ìœ„í•´ low-levelì€ ê³ ì •
3. **ê³„ì‚° íš¨ìœ¨ì„±**: Low-level policyì˜ gradient ê³„ì‚° ë¶ˆí•„ìš”
4. **ì•ˆì •ì„±**: Low-level policyê°€ ë³€ê²½ë˜ì§€ ì•Šì•„ í•™ìŠµì´ ë” ì•ˆì •ì 

### Frozen Policyì˜ ë™ì‘

```python
class FrozenLocomotionPolicy:
    def __call__(self, velocity_command):
        # 1. Low-level environmentì— velocity command ì„¤ì •
        env.command_manager.set_command("base_velocity", velocity_command)
        
        # 2. Observation ê°€ì ¸ì˜¤ê¸°
        obs = env.observation_manager.compute()
        
        # 3. Frozen policyë¡œ joint actions ìƒì„± (gradient ê³„ì‚° ì—†ìŒ)
        with torch.no_grad():
            joint_actions = self.inference_policy(obs)
        
        # 4. Original command ë³µì›
        return joint_actions
```

## ğŸ¯ í•™ìŠµ ëª©í‘œ

### Low-Level (ì´ë¯¸ í•™ìŠµ ì™„ë£Œ)
- **ëª©í‘œ**: ì£¼ì–´ì§„ velocity commandì— ë”°ë¼ ë¡œë´‡ì„ ì›€ì§ì´ëŠ” ê²ƒ
- **í•™ìŠµ ì™„ë£Œ**: Rough terrainì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ë³´í–‰ ê°€ëŠ¥

### High-Level (í˜„ì¬ í•™ìŠµ ì¤‘)
- **ëª©í‘œ**: ëª©í‘œ ìœ„ì¹˜ê¹Œì§€ íš¨ìœ¨ì ìœ¼ë¡œ ë„ë‹¬í•˜ëŠ” navigation ì „ëµ í•™ìŠµ
- **ì…ë ¥**: ë¡œë´‡ ìƒíƒœ + ëª©í‘œ ì •ë³´
- **ì¶œë ¥**: Velocity command (vx, vy, vyaw)
- **Reward**: Goal reaching + Progress

## ğŸ“ˆ í•™ìŠµ ê³¼ì •

### 1. Low-Level Policy ë¡œë“œ

```python
# ì‚¬ì „ í•™ìŠµëœ low-level policy ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
low_level_checkpoint = "logs/rsl_rl/deeprobotics_m20_rough/.../model_19999.pt"
ppo_runner.load(low_level_checkpoint)
inference_policy = ppo_runner.get_inference_policy()
freeze_policy(ppo_runner)  # Frozen ìƒíƒœë¡œ ì„¤ì •
```

### 2. High-Level Environment ìƒì„±

```python
# Low-level environment ìƒì„±
low_env = gym.make("Rough-Deeprobotics-M20-v0")

# Frozen policy wrapper ìƒì„±
frozen_policy = FrozenLocomotionPolicy(inference_policy, low_env)

# High-level environment ìƒì„±
hierarchical_env = HierarchicalNavEnv(
    env=low_env,
    frozen_policy_wrapper=frozen_policy,
    decimation=10
)
```

### 3. High-Level Policy í•™ìŠµ

```python
# High-level policy í•™ìŠµ (PPO)
runner = OnPolicyRunner(hierarchical_env, ...)
runner.learn(num_learning_iterations=20000)
```

## ğŸ”‘ ì£¼ìš” ê°œë… ì •ë¦¬

### 1. ê³„ì¸µ êµ¬ì¡°
- **High-Level**: ì¥ê¸° ëª©í‘œ (Navigation)
- **Low-Level**: ë‹¨ê¸° ì œì–´ (Locomotion)

### 2. Action Space ë¶„ë¦¬
- **High-Level**: Abstract actions (velocity commands)
- **Low-Level**: Primitive actions (joint torques)

### 3. Time Scale ë¶„ë¦¬
- **Decimation**: High-level 1 step = Low-level 10 steps

### 4. Policy ë¶„ë¦¬
- **High-Level Policy**: í•™ìŠµ ëŒ€ìƒ
- **Low-Level Policy**: Frozen (ê³ ì •)

## ğŸ“ ìš”ì•½

1. **High-Level Policy**ê°€ **velocity command**ë¥¼ ìƒì„±
2. **Frozen Low-Level Policy**ê°€ ì´ë¥¼ **joint actions**ë¡œ ë³€í™˜
3. **Low-Level Environment**ê°€ ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ (10íšŒ ë°˜ë³µ)
4. **High-Level**ì€ ëª©í‘œ ë„ë‹¬ì— ëŒ€í•œ **reward**ë¥¼ ë°›ê³  í•™ìŠµ

ì´ êµ¬ì¡°ë¥¼ í†µí•´:
- âœ… ë¡œë´‡ì˜ ê¸°ë³¸ ë³´í–‰ ëŠ¥ë ¥ì€ ìœ ì§€í•˜ë©´ì„œ
- âœ… Navigation ì „ëµë§Œ ë³„ë„ë¡œ í•™ìŠµ ê°€ëŠ¥
- âœ… ë” ë¹ ë¥´ê³  ì•ˆì •ì ì¸ í•™ìŠµ ê°€ëŠ¥

